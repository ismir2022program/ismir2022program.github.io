[
  {
    "content": {
      "TLDR": "The music industry attracts a large number of investors due to its high turnover. Releasing hit songs can garner profits, whereas flop songs lead to losses. Thus, predicting the popularity of a song before its release can help in promotion plans. Can we really predict hit Songs? This is the main motivation of the work. Extensive work is being done for western songs, but Indian music is relatively less explored. Hence, our work aims to predict hit songs of Indian origin using acoustic features. To analyze tracks, a data set is created from data provided by the Spotify Web API. The features are extracted using Spotify, available libraries such as Librosa and aubio, which are passed to machine learning algorithms for prediction. Along with available features, melodic features based on patterns are proposed and extracted. A comparative analysis is done for four acoustic feature sets containing timbral, pitch, rhythm and melodic features proposed. Further experimentation is performed with the combined features sets resulting in the improved performance. The results are encouraging and hit song prediction can be a reality in the near future.",
      "abstract": "The music industry attracts a large number of investors due to its high turnover. Releasing hit songs can garner profits, whereas flop songs lead to losses. Thus, predicting the popularity of a song before its release can help in promotion plans. Can we really predict hit Songs? This is the main motivation of the work. Extensive work is being done for western songs, but Indian music is relatively less explored. Hence, our work aims to predict hit songs of Indian origin using acoustic features. To analyze tracks, a data set is created from data provided by the Spotify Web API. The features are extracted using Spotify, available libraries such as Librosa and aubio, which are passed to machine learning algorithms for prediction. Along with available features, melodic features based on patterns are proposed and extracted. A comparative analysis is done for four acoustic feature sets containing timbral, pitch, rhythm and melodic features proposed. Further experimentation is performed with the combined features sets resulting in the improved performance. The results are encouraging and hit song prediction can be a reality in the near future.",
      "authors": [
        "Shreya Kale | Makarand Velankar | Rameshwari Joshi | Vaishnavi Ingole | Aparna Dhaygude"
      ],
      "bilibili_id": "",
      "channel_name": "",
      "channel_url": "",
      "day": 4,
      "poster_type": "",
      "session": [
        "Physical"
      ],
      "title": "Hit Song Prediction for Indian Popular Music",
      "youtube_id": ""
    },
    "forum": "362",
    "id": "362"
  },
  {
    "content": {
      "TLDR": "We investigate musical genre classification for three forms of Marathi vocal music each with a distinct socio-cultural context, namely devotional, poetic and folk dance. We present a dataset of songs covering the three genres, and discuss their musical and acoustic characteristics. We consider timbre and chroma based features for the 3-way classification. We specifically examine whether the source-separated vocal and instrumental accompaniment components can help improve genre recognition accuracy over that obtained with acoustic features extracted from the original mix audio track.",
      "abstract": "We investigate musical genre classification for three forms of Marathi vocal music each with a distinct socio-cultural context, namely devotional, poetic and folk dance. We present a dataset of songs covering the three genres, and discuss their musical and acoustic characteristics. We consider timbre and chroma based features for the 3-way classification. We specifically examine whether the source-separated vocal and instrumental accompaniment components can help improve genre recognition accuracy over that obtained with acoustic features extracted from the original mix audio track.",
      "authors": [
        "Shreyas M Nadkarni | Preeti Rao"
      ],
      "bilibili_id": "",
      "channel_name": "",
      "channel_url": "",
      "day": 4,
      "poster_type": "",
      "session": [
        "Physical"
      ],
      "title": "Genre Classification and Analysis of Marathi Songs",
      "youtube_id": ""
    },
    "forum": "365",
    "id": "365"
  },
  {
    "content": {
      "TLDR": "We contribute two design studies for augmented reality visualizations that support learning musical instruments. First, we designed simple, glanceable encodings for drum kits, which we display through a projector. As second instrument, we chose guitar and designed visualizations to be displayed either on a screen as an augmented mirror or an an optical see-through AR headset. These modalities allow us to also show information around the instrument and in 3D. We evaluated our prototypes through case studies and our results demonstrate the general effectivity and revealed design-related and technical limitations.",
      "abstract": "We contribute two design studies for augmented reality visualizations that support learning musical instruments. First, we designed simple, glanceable encodings for drum kits, which we display through a projector. As second instrument, we chose guitar and designed visualizations to be displayed either on a screen as an augmented mirror or an an optical see-through AR headset. These modalities allow us to also show information around the instrument and in 3D. We evaluated our prototypes through case studies and our results demonstrate the general effectivity and revealed design-related and technical limitations.",
      "authors": [
        "Frank Heyen | Michael Sedlmair"
      ],
      "bilibili_id": "",
      "channel_name": "",
      "channel_url": "",
      "day": 4,
      "poster_type": "",
      "session": [
        "Physical"
      ],
      "title": "Augmented Reality Visualization for Musical Instrument Learning",
      "youtube_id": ""
    },
    "forum": "376",
    "id": "376"
  },
  {
    "content": {
      "TLDR": "We contribute an interactive visual frontend to live coding environments, which allows live coders and performers to influence the behavior of their code more quickly and efficiently. Users can trigger actions and change parameters via instruments, buttons, and sliders, instead of only inside the code. For instance, toggling a loop or controlling a fading effect through mouse or touch interaction on a screen is faster than editing code. While this kind of control has already been possible with hardware MIDI devices, we provide a more accessible, easy-to-use, and customizable alternative that only requires a web browser. With examples, we show how users perform live-coded music faster and more easily with our design compared to using pure code.",
      "abstract": "We contribute an interactive visual frontend to live coding environments, which allows live coders and performers to influence the behavior of their code more quickly and efficiently. Users can trigger actions and change parameters via instruments, buttons, and sliders, instead of only inside the code. For instance, toggling a loop or controlling a fading effect through mouse or touch interaction on a screen is faster than editing code. While this kind of control has already been possible with hardware MIDI devices, we provide a more accessible, easy-to-use, and customizable alternative that only requires a web browser. With examples, we show how users perform live-coded music faster and more easily with our design compared to using pure code.",
      "authors": [
        "Frank Heyen | Michael Sedlmair"
      ],
      "bilibili_id": "",
      "channel_name": "",
      "channel_url": "",
      "day": 4,
      "poster_type": "",
      "session": [
        "Physical"
      ],
      "title": "A Web-Based MIDI Controller for Music Live Coding",
      "youtube_id": ""
    },
    "forum": "379",
    "id": "379"
  },
  {
    "content": {
      "TLDR": "In this paper, we propose a new paradigm to learn audio features for the task of Music Structure Analysis (MSA).We train a deep encoder to learn features such that the Self-Similarity-Matrix (SSM) resulting from those approximates a ground-truth SSM. This is done by minimizing a loss between both SSMs. Since this loss is differentiable w.r.t. its input features we can train the encoder in a straightforward way. We successfully demonstrate the use of this training paradigm using the AUC on the RWC-Pop dataset.",
      "abstract": "In this paper, we propose a new paradigm to learn audio features for the task of Music Structure Analysis (MSA).We train a deep encoder to learn features such that the Self-Similarity-Matrix (SSM) resulting from those approximates a ground-truth SSM. This is done by minimizing a loss between both SSMs. Since this loss is differentiable w.r.t. its input features we can train the encoder in a straightforward way. We successfully demonstrate the use of this training paradigm using the AUC on the RWC-Pop dataset.",
      "authors": [
        "Geoffroy Peeters |  Florian Angulo"
      ],
      "bilibili_id": "",
      "channel_name": "",
      "channel_url": "",
      "day": 4,
      "poster_type": "",
      "session": [
        "Physical"
      ],
      "title": "SSM-NET: Feature Learning for Music Structure Analysis Using a Self-similarity-matrix Based Loss",
      "youtube_id": ""
    },
    "forum": "381",
    "id": "381"
  },
  {
    "content": {
      "TLDR": "The pitch bend is used as a tool in a virtual instrument/synth that allows one to glide smoothly from one note to another. Almost all virtual instruments today employ the pitch bend joystick/wheel on a keyboard to solely manipulate the fundamental frequency or the pitch of the note being played. While this feature allows a perceptually realistic reproduction of slides in a few instruments (especially wind instruments), it largely fails to do so with plucked string instruments, especially non-Western traditional instruments. In this abstract, we chose to study the meend technique in Sitar performance in order to understand in finer detail the factors (including, but not limited to, the fundamental frequency) that characterize the glide between two notes.",
      "abstract": "The pitch bend is used as a tool in a virtual instrument/synth that allows one to glide smoothly from one note to another. Almost all virtual instruments today employ the pitch bend joystick/wheel on a keyboard to solely manipulate the fundamental frequency or the pitch of the note being played. While this feature allows a perceptually realistic reproduction of slides in a few instruments (especially wind instruments), it largely fails to do so with plucked string instruments, especially non-Western traditional instruments. In this abstract, we chose to study the meend technique in Sitar performance in order to understand in finer detail the factors (including, but not limited to, the fundamental frequency) that characterize the glide between two notes.",
      "authors": [
        "Suhit Chiruthapudi"
      ],
      "bilibili_id": "",
      "channel_name": "",
      "channel_url": "",
      "day": 4,
      "poster_type": "",
      "session": [
        "Physical"
      ],
      "title": "Differentiating the Pitch Bend Function Between the Sitar as a Real and Virtual Instrument",
      "youtube_id": ""
    },
    "forum": "383",
    "id": "383"
  },
  {
    "content": {
      "TLDR": "It has been shown in a recent publication that words in human-produced English language tend to have an information content close to the conditional entropy. In this paper, we show that the same is true for events in human-produced monophonic musical sequences. We also show how \"typical sampling\" influences the distribution of information around the entropy for single events and sequences.",
      "abstract": "It has been shown in a recent publication that words in human-produced English language tend to have an information content close to the conditional entropy. In this paper, we show that the same is true for events in human-produced monophonic musical sequences. We also show how \"typical sampling\" influences the distribution of information around the entropy for single events and sequences.",
      "authors": [
        "Mathais Rose Bjare | Stefan Lattner"
      ],
      "bilibili_id": "",
      "channel_name": "",
      "channel_url": "",
      "day": 4,
      "poster_type": "",
      "session": [
        "Physical"
      ],
      "title": "On the Typicality of Music",
      "youtube_id": ""
    },
    "forum": "385",
    "id": "385"
  },
  {
    "content": {
      "TLDR": "To date, little is known about the impact of different source locations on the listener's emotional response to music. Here we investigated through machine learning whether four music source locations (front, back, left, and right) could be accurately differentiated according to the type of valence in a subject-wise manner using spectral features extracted from electrophysiological (EEG) data. The results show that the four source locations can accurately be classified by different EEG correlates and that the effect is stronger when music characterized by negative emotional valence is played outside the listener's field of view. This proof-of-concept study may pave the way for advanced spatial audio analysis approaches in music information retrieval by taking into account the listener's emotional impact depending on the source direction of incidence.",
      "abstract": "To date, little is known about the impact of different source locations on the listener's emotional response to music. Here we investigated through machine learning whether four music source locations (front, back, left, and right) could be accurately differentiated according to the type of valence in a subject-wise manner using spectral features extracted from electrophysiological (EEG) data. The results show that the four source locations can accurately be classified by different EEG correlates and that the effect is stronger when music characterized by negative emotional valence is played outside the listener's field of view. This proof-of-concept study may pave the way for advanced spatial audio analysis approaches in music information retrieval by taking into account the listener's emotional impact depending on the source direction of incidence.",
      "authors": [
        "Eleonora De Filippi | Timothy Schmele | ARIJIT NANDI"
      ],
      "bilibili_id": "",
      "channel_name": "",
      "channel_url": "",
      "day": 4,
      "poster_type": "",
      "session": [
        "Physical"
      ],
      "title": "Towards a machine-learning approach to analyse the emotional impact of source localization in music",
      "youtube_id": ""
    },
    "forum": "386",
    "id": "386"
  },
  {
    "content": {
      "TLDR": "Music structure analysis (MSA) systems aim to segment a song recording into non-overlapping sections with useful labels. Previous MSA systems typically predict abstract labels in a post-processing step and require the full context of the song. By contrast, we recently proposed a supervised framework, called \"Music Structural Function Analysis\" (MuSFA), that models and predicts meaningful labels like 'verse' and 'chorus' directly from audio, without requiring the full context of a song. However, the performance of this system depends on the amount and quality of training data. In this paper, we propose to repurpose a public dataset, HookTheory Lead Sheet Dataset (HLSD), to improve the performance. HLSD contains over 18K excerpts of music sections originally collected for studying automatic melody harmonization. We treat each excerpt as a partially labeled song and provide a label mapping, so that HLSD can be used together with other public datasets, such as SALAMI, RWC, and Isophonics. In cross-dataset evaluations, we find that including HLSD in training can improve state-of-the-art boundary detection and section labeling scores by ~3% and ~1% respectively.",
      "abstract": "Music structure analysis (MSA) systems aim to segment a song recording into non-overlapping sections with useful labels. Previous MSA systems typically predict abstract labels in a post-processing step and require the full context of the song. By contrast, we recently proposed a supervised framework, called \"Music Structural Function Analysis\" (MuSFA), that models and predicts meaningful labels like 'verse' and 'chorus' directly from audio, without requiring the full context of a song. However, the performance of this system depends on the amount and quality of training data. In this paper, we propose to repurpose a public dataset, HookTheory Lead Sheet Dataset (HLSD), to improve the performance. HLSD contains over 18K excerpts of music sections originally collected for studying automatic melody harmonization. We treat each excerpt as a partially labeled song and provide a label mapping, so that HLSD can be used together with other public datasets, such as SALAMI, RWC, and Isophonics. In cross-dataset evaluations, we find that including HLSD in training can improve state-of-the-art boundary detection and section labeling scores by ~3% and ~1% respectively.",
      "authors": [
        "Ju-Chiang Wang | Jordan B. L. Smith | Yun-Ning Hung"
      ],
      "bilibili_id": "",
      "channel_name": "",
      "channel_url": "",
      "day": 4,
      "poster_type": "",
      "session": [
        "Physical"
      ],
      "title": "MuSFA: Improving Music Structural Function Analysis with Partially Labeled Data",
      "youtube_id": ""
    },
    "forum": "389",
    "id": "389"
  },
  {
    "content": {
      "TLDR": "The Audio Metaphor (AUME) demonstration invites participants to explore an interactive system for automatic soundscape composition. AUME is built as an online prompt-based system which produces audio soundscapes given a textual and affective query. The user can set valence and arousal curves which the system uses to generate a soundscape composition matching the desired eventfulness and mood. By streamlining the composition process, AUME aims to relieve some of the cognitive work done by sound designers. AUME's latest iteration draws from a database of nearly half a million audio files, each paired with a list of textual sound descriptors. Along with this expanded capacity, comes a refined ability to interpret a wider lexicon describing sound. Improvements to AUME\u2019s algorithms and architecture for sound retrieval, segmentation, background and foreground classification, automatic mixing and automatic soundscape affect recognition, makes it a powerful system that generates believable soundscapes at interactive rates.",
      "abstract": "The Audio Metaphor (AUME) demonstration invites participants to explore an interactive system for automatic soundscape composition. AUME is built as an online prompt-based system which produces audio soundscapes given a textual and affective query. The user can set valence and arousal curves which the system uses to generate a soundscape composition matching the desired eventfulness and mood. By streamlining the composition process, AUME aims to relieve some of the cognitive work done by sound designers. AUME's latest iteration draws from a database of nearly half a million audio files, each paired with a list of textual sound descriptors. Along with this expanded capacity, comes a refined ability to interpret a wider lexicon describing sound. Improvements to AUME\u2019s algorithms and architecture for sound retrieval, segmentation, background and foreground classification, automatic mixing and automatic soundscape affect recognition, makes it a powerful system that generates believable soundscapes at interactive rates.",
      "authors": [
        "Renaud Bougueng Tchemeube | Joshua Kranabetter | Craig Carpenter | Philippe Pasquier | Miles Thorogood"
      ],
      "bilibili_id": "",
      "channel_name": "",
      "channel_url": "",
      "day": 4,
      "poster_type": "",
      "session": [
        "Physical"
      ],
      "title": "Audio Metaphor 2.0: An Improved System for Automatic Sound Design",
      "youtube_id": ""
    },
    "forum": "390",
    "id": "390"
  },
  {
    "content": {
      "TLDR": "With the rise of artificial intelligence has come an increase in its application towards creative domains, including music. Many systems have been built that apply machine learning approaches to the problem of computer-assisted music composition (CAC). Calliope is a web application that assists composers in performing multi-track composition tasks in the symbolic domain. The composer can upload Musical Instrument Digital Interface (MIDI) files, visualize and edit MIDI tracks, and generate partial (via bar in-filling) or complete multi-track content using the Multi-Track Music Machine (MMM). Generation can be done in batch, and can be combined with active playback listening and ranking for an enhanced workflow. Generated MIDI files can be exported or streamed to any standard Digital Audio Workstation (DAW). We present the system\u2019s features and describe its co-creative workflow. Calliope can be used for creative ideation, musical exploration or for full multi-track music compositions.",
      "abstract": "With the rise of artificial intelligence has come an increase in its application towards creative domains, including music. Many systems have been built that apply machine learning approaches to the problem of computer-assisted music composition (CAC). Calliope is a web application that assists composers in performing multi-track composition tasks in the symbolic domain. The composer can upload Musical Instrument Digital Interface (MIDI) files, visualize and edit MIDI tracks, and generate partial (via bar in-filling) or complete multi-track content using the Multi-Track Music Machine (MMM). Generation can be done in batch, and can be combined with active playback listening and ranking for an enhanced workflow. Generated MIDI files can be exported or streamed to any standard Digital Audio Workstation (DAW). We present the system\u2019s features and describe its co-creative workflow. Calliope can be used for creative ideation, musical exploration or for full multi-track music compositions.",
      "authors": [
        "Renaud Bougueng Tchemeube | Jeffrey Ens | Cale Plut | Philippe Pasquier"
      ],
      "bilibili_id": "",
      "channel_name": "",
      "channel_url": "",
      "day": 4,
      "poster_type": "",
      "session": [
        "Physical"
      ],
      "title": "Calliope: An Online Interface for Generative Music Co-Creation",
      "youtube_id": ""
    },
    "forum": "391",
    "id": "391"
  },
  {
    "content": {
      "TLDR": "FixMatch, a semi-supervised learning method proposed for image classification, includes unlabeled data instances into the training procedure by predicting labels for differently augmented versions of the unlabeled data. In our previous work, we adapted FixMatch to audio classification by applying image augmentations to spectral representations of the audio signal. While this approach matched the performance of the supervised baseline with only a fraction of the training data, the performance of audio-specific augmentation techniques, and their effect on the FixMatch approach was not evaluated. In this work, we replace all image-based augmentation techniques with audio-specific ones and keep the feature extraction unchanged. The audio-specific approach improved upon the supervised baseline which confirms the effectiveness of the FixMatch approach for semi-supervised learning even with a completely different set of augmentations. However, the image-based approach outperforms the audio-based approach on the three audio classification tasks evaluated.",
      "abstract": "FixMatch, a semi-supervised learning method proposed for image classification, includes unlabeled data instances into the training procedure by predicting labels for differently augmented versions of the unlabeled data. In our previous work, we adapted FixMatch to audio classification by applying image augmentations to spectral representations of the audio signal. While this approach matched the performance of the supervised baseline with only a fraction of the training data, the performance of audio-specific augmentation techniques, and their effect on the FixMatch approach was not evaluated. In this work, we replace all image-based augmentation techniques with audio-specific ones and keep the feature extraction unchanged. The audio-specific approach improved upon the supervised baseline which confirms the effectiveness of the FixMatch approach for semi-supervised learning even with a completely different set of augmentations. However, the image-based approach outperforms the audio-based approach on the three audio classification tasks evaluated.",
      "authors": [
        "Sascha Grollmisch | Estefan\u00eda Cano | Jakob Abe\u00dfer"
      ],
      "bilibili_id": "",
      "channel_name": "",
      "channel_url": "",
      "day": 4,
      "poster_type": "",
      "session": [
        "Virtual"
      ],
      "title": "Audio Augmentations for Semi-Supervised Learning with FixMatch",
      "youtube_id": ""
    },
    "forum": "363",
    "id": "363"
  },
  {
    "content": {
      "TLDR": "The extraction of fundamental frequency (F0) information from music recordings is a crucial task in the field of music information retrieval. The sequence of F0-estimates over successive time frames (also called F0-trajectory) often corresponds to a melodic phrase and serves as a representation for downstream tasks such as automatic music transcription and performance analysis. A large number of algorithms and tools for F0-estimation have been proposed in the literature and implemented in various programming languages. However, these heterogeneous implementations are often not easily comparable and may vary considerably in performance and accuracy, which is problematic for reproducible research. In this contribution, we introduce libf0, a Python library of reference implementations that can conveniently be used to apply, compare, and develop F0-estimation algorithms in a reproducible way.",
      "abstract": "The extraction of fundamental frequency (F0) information from music recordings is a crucial task in the field of music information retrieval. The sequence of F0-estimates over successive time frames (also called F0-trajectory) often corresponds to a melodic phrase and serves as a representation for downstream tasks such as automatic music transcription and performance analysis. A large number of algorithms and tools for F0-estimation have been proposed in the literature and implemented in various programming languages. However, these heterogeneous implementations are often not easily comparable and may vary considerably in performance and accuracy, which is problematic for reproducible research. In this contribution, we introduce libf0, a Python library of reference implementations that can conveniently be used to apply, compare, and develop F0-estimation algorithms in a reproducible way.",
      "authors": [
        "Sebastian Rosenzweig | Simon J Schw\u00e4r | Meinard M\u00fcller"
      ],
      "bilibili_id": "",
      "channel_name": "",
      "channel_url": "",
      "day": 4,
      "poster_type": "",
      "session": [
        "Virtual"
      ],
      "title": "libf0: A Python Library for Fundamental Frequency Estimation",
      "youtube_id": ""
    },
    "forum": "364",
    "id": "364"
  },
  {
    "content": {
      "TLDR": "EGFxSet contains recordings of all clean tones in a Stratocaster guitar, with augmentations by processing through twelve electric guitar effects. Similar datasets apply effects using software, EGFxSet in contrast uses real guitar effects hardware, making it relevant to develop MIR tools with applications on real music. Annotations include all guitar and effect parameters controlled during our dataset recording. EGFxSet contains 8970 unique, annotated guitar tones, and is published with full open-access rights.",
      "abstract": "EGFxSet contains recordings of all clean tones in a Stratocaster guitar, with augmentations by processing through twelve electric guitar effects. Similar datasets apply effects using software, EGFxSet in contrast uses real guitar effects hardware, making it relevant to develop MIR tools with applications on real music. Annotations include all guitar and effect parameters controlled during our dataset recording. EGFxSet contains 8970 unique, annotated guitar tones, and is published with full open-access rights.",
      "authors": [
        "Hegel Emmanuel Pedroza | Gerardo Meza | Iran R Roman"
      ],
      "bilibili_id": "",
      "channel_name": "",
      "channel_url": "",
      "day": 4,
      "poster_type": "",
      "session": [
        "Virtual"
      ],
      "title": "EGFxSet: Electric Guitar Tones Processed Through Real Effects of Distortion, Modulation, Delay and Reverb",
      "youtube_id": ""
    },
    "forum": "367",
    "id": "367"
  },
  {
    "content": {
      "TLDR": "Language models have made great progress in symbolic music generation. However, to the best of our knowledge, without large human-annotated datasets, music generation constrained to a predefined musical form has not been well studied. In this demo paper, we present a GPT2-based melody generation system for generating well-structured melodies via controllable similarity and length. We design several embeddings and tokens to modelling similarity and length information within the music, and train the GPT-2 model with a large dataset in ABC notation. With preliminary experimental results and examples of self-similarity matrices, we demonstrate the potential of this system for generating melodies with long-term repetitive structures.",
      "abstract": "Language models have made great progress in symbolic music generation. However, to the best of our knowledge, without large human-annotated datasets, music generation constrained to a predefined musical form has not been well studied. In this demo paper, we present a GPT2-based melody generation system for generating well-structured melodies via controllable similarity and length. We design several embeddings and tokens to modelling similarity and length information within the music, and train the GPT-2 model with a large dataset in ABC notation. With preliminary experimental results and examples of self-similarity matrices, we demonstrate the potential of this system for generating melodies with long-term repetitive structures.",
      "authors": [
        "Shangda Wu | Yuanliang Dong | Maosong Sun"
      ],
      "bilibili_id": "",
      "channel_name": "",
      "channel_url": "",
      "day": 4,
      "poster_type": "",
      "session": [
        "Virtual"
      ],
      "title": "Generating Melodies with Controllable Similarity and Length in ABC Notation",
      "youtube_id": ""
    },
    "forum": "368",
    "id": "368"
  },
  {
    "content": {
      "TLDR": "Training sequence models such as transformers with symbolic music necessitates a representation of music as sequences of atomic elements called tokens. State-of-the-art music tokenizations encode pitch values explicitly, which complicates the ability of a machine learning model to generalize musical knowledge at different keys. We propose tracks for a tokenization encoding pitch intervals rather than pitch values, resulting in transposition invariant representations. The musical expressivity of this new tokenization is evaluated through two MIR classification tasks: composer classification and end of phrase detection.",
      "abstract": "Training sequence models such as transformers with symbolic music necessitates a representation of music as sequences of atomic elements called tokens. State-of-the-art music tokenizations encode pitch values explicitly, which complicates the ability of a machine learning model to generalize musical knowledge at different keys. We propose tracks for a tokenization encoding pitch intervals rather than pitch values, resulting in transposition invariant representations. The musical expressivity of this new tokenization is evaluated through two MIR classification tasks: composer classification and end of phrase detection.",
      "authors": [
        "Louis Bigo | Mikaela Keller"
      ],
      "bilibili_id": "",
      "channel_name": "",
      "channel_url": "",
      "day": 4,
      "poster_type": "",
      "session": [
        "Virtual"
      ],
      "title": "Improving tokenization expressiveness with pitch intervals",
      "youtube_id": ""
    },
    "forum": "369",
    "id": "369"
  },
  {
    "content": {
      "TLDR": "We created a visualization tool that helps Automatic Chord Recognition (ACR) developers to characterize system performance across a test data set. Our system's design uses Information Visualization (InfoVis) principles to communicate accuracy more effectively than a table of mean metric scores. We share some of the insights we developed while building our tool, and hope our findings may help inform the design of figures used in future publications, and affect how future ACR system designers improve and present their systems.",
      "abstract": "We created a visualization tool that helps Automatic Chord Recognition (ACR) developers to characterize system performance across a test data set. Our system's design uses Information Visualization (InfoVis) principles to communicate accuracy more effectively than a table of mean metric scores. We share some of the insights we developed while building our tool, and hope our findings may help inform the design of figures used in future publications, and affect how future ACR system designers improve and present their systems.",
      "authors": [
        "Christopher Liscio |  Dan Brown"
      ],
      "bilibili_id": "",
      "channel_name": "",
      "channel_url": "",
      "day": 4,
      "poster_type": "",
      "session": [
        "Virtual"
      ],
      "title": "Visualizing Chord Recognition Performance",
      "youtube_id": ""
    },
    "forum": "370",
    "id": "370"
  },
  {
    "content": {
      "TLDR": "A monophonic instrument can play at once several voices or lines, for example when interleaving pedal notes and scales. Such patterns may bear both melodic, harmonic, and rhythmic elements and are frequent in cello music. We propose a model of alternating patterns, where regularly spaced pitches are linked with some relation. We also propose an algorithm to list all such patterns. Perspectives include better corpus analysis, and extending and benchmarking such algorithms.",
      "abstract": "A monophonic instrument can play at once several voices or lines, for example when interleaving pedal notes and scales. Such patterns may bear both melodic, harmonic, and rhythmic elements and are frequent in cello music. We propose a model of alternating patterns, where regularly spaced pitches are linked with some relation. We also propose an algorithm to list all such patterns. Perspectives include better corpus analysis, and extending and benchmarking such algorithms.",
      "authors": [
        "Perrine Vantalon | Mathieu Giraud | Richard Groult | Thierry Lecroq"
      ],
      "bilibili_id": "",
      "channel_name": "",
      "channel_url": "",
      "day": 4,
      "poster_type": "",
      "session": [
        "Virtual"
      ],
      "title": "Towards modeling alternating patterns through inter-notes relations",
      "youtube_id": ""
    },
    "forum": "371",
    "id": "371"
  },
  {
    "content": {
      "TLDR": "Popularity bias is the idea that a music recommender system will unduly favor popular artists when recommending artists to users. In this paper, we attempt to measure popularity bias on three commercial music streaming services (Spotify, Amazon Music, YouTube). We find no significant evidence of popularity bias in the commercial recommendations based on a simulated user experiment.",
      "abstract": "Popularity bias is the idea that a music recommender system will unduly favor popular artists when recommending artists to users. In this paper, we attempt to measure popularity bias on three commercial music streaming services (Spotify, Amazon Music, YouTube). We find no significant evidence of popularity bias in the commercial recommendations based on a simulated user experiment.",
      "authors": [
        "Douglas Turnbull | Vera Crabtree | Sean McQuillan"
      ],
      "bilibili_id": "",
      "channel_name": "",
      "channel_url": "",
      "day": 4,
      "poster_type": "",
      "session": [
        "Virtual"
      ],
      "title": "Exploring Popularity Bias in Music Steaming Services",
      "youtube_id": ""
    },
    "forum": "372",
    "id": "372"
  },
  {
    "content": {
      "TLDR": "Expert musicians can mould a musical piece to convey specific emotions that they intend to communicate. In this paper, we place a mid-level features based music emotion model in this performer-to-listener communication scenario, and demonstrate via a small visualisation music emotion decoding in real time. We also extend the existing set of mid-level features using analogues of perceptual speed and perceived dynamics.",
      "abstract": "Expert musicians can mould a musical piece to convey specific emotions that they intend to communicate. In this paper, we place a mid-level features based music emotion model in this performer-to-listener communication scenario, and demonstrate via a small visualisation music emotion decoding in real time. We also extend the existing set of mid-level features using analogues of perceptual speed and perceived dynamics.",
      "authors": [
        "Shreyan Chowdhury | Gerhard Widmer"
      ],
      "bilibili_id": "",
      "channel_name": "",
      "channel_url": "",
      "day": 4,
      "poster_type": "",
      "session": [
        "Virtual"
      ],
      "title": "Decoding and Visualising Intended Emotion in an Expressive Piano Performance",
      "youtube_id": ""
    },
    "forum": "374",
    "id": "374"
  },
  {
    "content": {
      "TLDR": "We introduce the Classical Concert Video Shot (CCVS) dataset with concert videos of classical music performance and annotations available for the MIR research on concert videography. In this dataset, we annotate the start time, end time, shot class and musical instruments of each shot in the video. Eight classes of video shot and 33 classes of instruments are considered in the annotation. Totally 5,527 shot tags with instruments are then collected from 207 YouTube videos. The total length of the dataset reaches 12.68 hours.",
      "abstract": "We introduce the Classical Concert Video Shot (CCVS) dataset with concert videos of classical music performance and annotations available for the MIR research on concert videography. In this dataset, we annotate the start time, end time, shot class and musical instruments of each shot in the video. Eight classes of video shot and 33 classes of instruments are considered in the annotation. Totally 5,527 shot tags with instruments are then collected from 207 YouTube videos. The total length of the dataset reaches 12.68 hours.",
      "authors": [
        "Hsin-Min Chou | Ting-Wei Lin | Jen-Chun Lin | Ching Te Chiu | Li Su (Academia Sinica)"
      ],
      "bilibili_id": "",
      "channel_name": "",
      "channel_url": "",
      "day": 4,
      "poster_type": "",
      "session": [
        "Virtual"
      ],
      "title": "CCVS: A dataset for concert videography research",
      "youtube_id": ""
    },
    "forum": "375",
    "id": "375"
  },
  {
    "content": {
      "TLDR": "AudioLoader is a PyTorch package which helps users to auto-download, unzip and prepossess (audio re-sampling, segmenting, data splitting) common audio datasets that are still not available in the official torchaudio dataset collection yet. AudioLoader supports a wide rage of datasets for different applications such as speech recognition (Multilingual LibriSpeech (MLS), TIMIT, SpeechCommands v2 with 12 classes), automatic music transcription (MAPS, MusicNet, MAESTRO), and music source separation (MusdbHQ). Slakh2100 will also be included in our future release. \n \n AudioLoader is designed to be hassle-free. Once called, it returns a pytorch dataset class, and it can be combined with \\verb | torch.utils.data.DataLoader |  as usual. This design allows users and researchers to spend less time on dataset preparation so that they can focus more on the research and model development part. In this paper, we will demonstrate the usage of AudioLoader by using various datasets such as MLS, MAESTEO, and MusdbHQ as the examples. AudioLoader is an on-going open source project available on GitHub, more datasets will be supported in the future and contributions from the community is highly welcomed.",
      "abstract": "AudioLoader is a PyTorch package which helps users to auto-download, unzip and prepossess (audio re-sampling, segmenting, data splitting) common audio datasets that are still not available in the official torchaudio dataset collection yet. AudioLoader supports a wide rage of datasets for different applications such as speech recognition (Multilingual LibriSpeech (MLS), TIMIT, SpeechCommands v2 with 12 classes), automatic music transcription (MAPS, MusicNet, MAESTRO), and music source separation (MusdbHQ). Slakh2100 will also be included in our future release. \n \n AudioLoader is designed to be hassle-free. Once called, it returns a pytorch dataset class, and it can be combined with \\verb | torch.utils.data.DataLoader |  as usual. This design allows users and researchers to spend less time on dataset preparation so that they can focus more on the research and model development part. In this paper, we will demonstrate the usage of AudioLoader by using various datasets such as MLS, MAESTEO, and MusdbHQ as the examples. AudioLoader is an on-going open source project available on GitHub, more datasets will be supported in the future and contributions from the community is highly welcomed.",
      "authors": [
        "Kin Wai Cheuk | Kwan Yee Heung | Dorien Herremans"
      ],
      "bilibili_id": "",
      "channel_name": "",
      "channel_url": "",
      "day": 4,
      "poster_type": "",
      "session": [
        "Virtual"
      ],
      "title": "AudioLoader: a hassle-free Pytorch audio dataset loader",
      "youtube_id": ""
    },
    "forum": "377",
    "id": "377"
  },
  {
    "content": {
      "TLDR": "Gamaka (note ornamentation) is an essential element of Carnatic music. Earlier works in computer-generated gamakas focused on developing mathematical models of each gamaka, which fails to capture the intricate changes in pitch and thus does not sound natural. To address this challenge, this work approaches the synthesis of gamaka for kalpitha swaras (composed notes) in Carnatic music using a data-driven system. The model uses masked latent space representation in an auto-encoder architecture with features extracted using convolutional layers. \n It takes as input, the pitch contour extracted from symbolic data to generate a pitch contour with gamaka information embedded in it. The model is successful in synthesizing gamaka with nuances that closely follow the ground truth.",
      "abstract": "Gamaka (note ornamentation) is an essential element of Carnatic music. Earlier works in computer-generated gamakas focused on developing mathematical models of each gamaka, which fails to capture the intricate changes in pitch and thus does not sound natural. To address this challenge, this work approaches the synthesis of gamaka for kalpitha swaras (composed notes) in Carnatic music using a data-driven system. The model uses masked latent space representation in an auto-encoder architecture with features extracted using convolutional layers. \n It takes as input, the pitch contour extracted from symbolic data to generate a pitch contour with gamaka information embedded in it. The model is successful in synthesizing gamaka with nuances that closely follow the ground truth.",
      "authors": [
        "Raghavasimhan Sankaranarayanan | Gil Weinberg"
      ],
      "bilibili_id": "",
      "channel_name": "",
      "channel_url": "",
      "day": 4,
      "poster_type": "",
      "session": [
        "Virtual"
      ],
      "title": "Gamaka Synthesis for Kalpitha Swaras in Carnatic music",
      "youtube_id": ""
    },
    "forum": "380",
    "id": "380"
  },
  {
    "content": {
      "TLDR": "Music recommendation systems are researched from multiple aspects, and the recent focus has been on the relationship between physiological measures and music preference. These studies, however, face the challenge of varying acoustic features among stimuli, which can con-found with the effect of preference. Also, access to physiological signals is often limited in real-life applications such as smart devices. In this study, we aimed to reduce the effect of acoustic variability by presenting different expressions of the same musical piece while connecting the study to daily use by shortening the stimulus length. We predicted participants\u2019 preference for musical expressions from cardiac and respiratory data measured from 30 subjects in a psychophysiological experiment. We identified a non-linear relationship between physiological signals and musical preference over an ultrashort time interval (~15 s). This result suggests that music recommendation systems can use biological signals to adapt their model rapidly on minimal data retrieval.",
      "abstract": "Music recommendation systems are researched from multiple aspects, and the recent focus has been on the relationship between physiological measures and music preference. These studies, however, face the challenge of varying acoustic features among stimuli, which can con-found with the effect of preference. Also, access to physiological signals is often limited in real-life applications such as smart devices. In this study, we aimed to reduce the effect of acoustic variability by presenting different expressions of the same musical piece while connecting the study to daily use by shortening the stimulus length. We predicted participants\u2019 preference for musical expressions from cardiac and respiratory data measured from 30 subjects in a psychophysiological experiment. We identified a non-linear relationship between physiological signals and musical preference over an ultrashort time interval (~15 s). This result suggests that music recommendation systems can use biological signals to adapt their model rapidly on minimal data retrieval.",
      "authors": [
        "Shu Sakamoto |  Vincent Cheung | Shinichi Furuya"
      ],
      "bilibili_id": "",
      "channel_name": "",
      "channel_url": "",
      "day": 4,
      "poster_type": "",
      "session": [
        "Virtual"
      ],
      "title": "Rapidly Predicting Music Artistic Expression Preference From Heart Rate and Respiration Rate",
      "youtube_id": ""
    },
    "forum": "388",
    "id": "388"
  },
  {
    "content": {
      "TLDR": "This paper presents the wmn4j Java library for handling Western music notation. The central goal of wmn4j is to provide a simple model of musical scores and an intuitive API that allows efficient access to their contents. Wmn4j supports fully concurrent and parallel access to all contents of scores and is intended for implementing large scale server-side music analysis applications. Wmn4j is licensed under the MIT license and is available on Github and Maven Central.",
      "abstract": "This paper presents the wmn4j Java library for handling Western music notation. The central goal of wmn4j is to provide a simple model of musical scores and an intuitive API that allows efficient access to their contents. Wmn4j supports fully concurrent and parallel access to all contents of scores and is intended for implementing large scale server-side music analysis applications. Wmn4j is licensed under the MIT license and is available on Github and Maven Central.",
      "authors": [
        "Otso Bj\u00f6rklund"
      ],
      "bilibili_id": "",
      "channel_name": "",
      "channel_url": "",
      "day": 4,
      "poster_type": "",
      "session": [
        "Virtual"
      ],
      "title": "Western Music Notation for Java: A library for music notation on the JVM",
      "youtube_id": ""
    },
    "forum": "392",
    "id": "392"
  }
]
